# HeterogeneousParallelization_Assignment4

# Assignment 4: Гибридные и распределённые параллельные вычисления

**Astana IT University**

**Course:** Heterogeneous Parallelization

**Course instructor:** Садвакасова Куралай Жанжигитовна

**Student:** Жумагулова Карина

**Group:** ADA-2403M

**Date:** 25.01.2026

Данный репозиторий содержит решения Assignment 4, посвящённого изучению **гибридных и распределённых параллельных вычислений**. Работа демонстрирует:

* использование **GPU с глобальной и shared памятью**,
* реализацию **гибридной программы на CPU+GPU**,
* распределённые вычисления с использованием **MPI**,
* измерение времени выполнения и сравнение различных подходов.

## Структура репозитория

```
.
├── HP_Assignment_4.ipynb          # Объединённые заметки и тесты всех заданий
├── task_1.cu                      # Задание 1: Сумма элементов массива на GPU (global memory)
├── task_2.cu                      # Задание 2: Префиксная сумма с shared memory
├── task_3_1.cu                    # Задание 3: Гибридная обработка массива CPU+GPU
├── task_4.cpp                     # Задание 4: Распределённая обработка массива с MPI
├── questions.md                  # Ответы на контрольные вопросы Assignment 4
├── README.md                     # Описание проекта и структура репозитория
```

## Задание 1 (task_1.cu)

### Описание

В данном задании реализована CUDA-программа для вычисления **суммы элементов массива** с использованием **глобальной памяти GPU**.
Вычисления выполняются двумя способами:

1. **Последовательная реализация на CPU**
2. **Параллельная реализация на GPU** с использованием CUDA-ядра и атомарных операций

Для оценки эффективности параллельных вычислений производится сравнение **результата и времени выполнения** CPU- и GPU-реализаций для массива размером **100 000 элементов**.

### Цель задания

* Изучить базовые принципы параллельных вычислений на GPU с использованием CUDA.
* Реализовать параллельное суммирование элементов массива в **глобальной памяти GPU**.
* Ознакомиться с использованием **атомарных операций (`atomicAdd`)** для корректного обновления общей суммы.
* Сравнить производительность **последовательной CPU-реализации** и **параллельной GPU-реализации**.
* Научиться измерять время выполнения CPU- и GPU-вычислений.

### Функционал программы

1. **Инициализация данных**

   * Создание массива из `100 000` целых чисел.
   * Заполнение массива случайными значениями в диапазоне от `-10 000` до `10 000`.
   * Вывод первых и последних 10 элементов массива для проверки корректности данных.

2. **Последовательное суммирование на CPU**

   * Суммирование элементов массива в одном потоке.
   * Измерение времени выполнения с помощью `std::chrono`.

3. **Параллельное суммирование на GPU**

   * Выделение памяти в глобальной памяти GPU.
   * Копирование массива с CPU (Host) на GPU (Device).
   * Запуск CUDA-ядра, в котором каждый поток:

     * обрабатывает один элемент массива,
     * добавляет его значение к общей сумме с помощью `atomicAdd`.
   * Копирование результата обратно на CPU.
   * Измерение времени выполнения GPU-ядра с помощью `cudaEvent_t`.

4. **Сравнение результатов**

   * Вывод суммы, вычисленной на CPU и GPU.
   * Вывод времени выполнения CPU- и GPU-реализаций.
   * Проверка совпадения результатов суммирования.

### Используемые технологии

* **C++**
* **CUDA**
* **Global memory GPU**
* **CUDA kernels**
* **atomicAdd** — атомарная операция сложения
* **cudaMalloc / cudaMemcpy / cudaFree** — управление памятью GPU
* **cudaEvent_t** — измерение времени выполнения GPU-вычислений
* **std::chrono** — измерение времени выполнения на CPU
* **std::vector** — хранение массива данных
* **std::random / Mersenne Twister** — генерация случайных чисел

## Assignment 4 (task_2.cu)

### Prefix Sum (Scan) Using CUDA Shared Memory

### Цель задания

Реализовать CUDA-программу для вычисления **префиксной суммы (scan)** массива с использованием **разделяемой памяти (shared memory)** и сравнить время выполнения с **последовательной реализацией на CPU** для массива размером **1 000 000 элементов**.

### Описание задачи

Префиксная сумма массива — это операция, при которой каждый элемент выходного массива содержит сумму всех элементов входного массива от начала до текущего индекса:

[
output[i] = \sum_{k=0}^{i} input[k]
]

В рамках задания:

* реализована последовательная версия алгоритма на CPU;
* реализована параллельная версия на GPU с использованием CUDA;
* выполнено сравнение времени выполнения CPU и GPU реализаций.

### Подход к реализации

#### CPU-реализация

На CPU используется стандартный последовательный алгоритм:

* первый элемент копируется без изменений;
* каждый следующий элемент вычисляется как сумма предыдущего результата и текущего входного значения.

Алгоритм имеет вычислительную сложность **O(N)**.

#### GPU-реализация (CUDA)

GPU-реализация состоит из **трёх этапов**:

##### 1. Внутриблочное сканирование

* Массив разбивается на блоки по `BLOCK_SIZE = 256` элементов.
* Каждый блок загружается в **shared memory**.
* Внутри блока выполняется **инклюзивное сканирование** с использованием алгоритма **Hillis–Steele**.
* Последний поток каждого блока сохраняет сумму элементов блока в отдельный массив `block_sums`.

##### 2. Сканирование сумм блоков

* Так как количество блоков значительно меньше общего числа элементов, префиксная сумма массива `block_sums` вычисляется **на CPU**.
* Полученный массив `block_sums_scan` содержит накопленные суммы всех предыдущих блоков.
* Результат копируется обратно на GPU.

##### 3. Коррекция результатов

* Для всех блоков, кроме первого, к элементам добавляется сумма всех предыдущих блоков.
* Это позволяет получить корректную глобальную префиксную сумму всего массива.

### Используемые технологии

* **CUDA C++**
* **Shared Memory**
* **CUDA Kernels**
* **CUDA Events** для замера времени
* **C++ STL** (`vector`, `chrono`, `random`)

### Параметры эксперимента

| Параметр         | Значение    |
| ---------------- | ----------- |
| Размер массива   | 1 000 000   |
| Размер блока     | 256 потоков |
| Тип сканирования | Инклюзивное |
| Тип данных       | `int`       |


### Проверка корректности

Для проверки корректности:

* выводятся первые и последние 10 элементов входного массива;
* сравниваются первые и последние 10 элементов префиксной суммы, вычисленной на CPU и GPU.

Совпадение результатов подтверждает корректность реализации.

### Результаты

В программе измеряется:

* время выполнения последовательной версии на CPU;
* общее время выполнения GPU-реализации (включая запуск ядер и копирование данных).

Ожидаемый результат:

* GPU-версия демонстрирует **значительное ускорение** по сравнению с CPU для массива большого размера;
* эффективность достигается за счёт использования **разделяемой памяти** и параллельного выполнения.

### Вывод

В ходе выполнения задания была успешно реализована параллельная префиксная сумма на GPU с использованием shared memory. Эксперимент показал, что CUDA позволяет существенно ускорить операции сканирования для больших массивов данных по сравнению с последовательной CPU-реализацией.

## Assignment 4 (task_3_1.cu)

### Hybrid CPU + GPU Array Processing

### Цель задания

Реализовать **гибридную параллельную программу**, в которой обработка массива выполняется **одновременно на CPU и GPU**, и сравнить время выполнения:

* последовательной CPU-реализации;
* параллельной GPU-реализации;
* гибридного подхода (CPU + GPU).

Размер массива: **1 000 000 элементов**.

### Описание задачи

В данном задании исследуется эффективность гибридных вычислений, при которых вычислительная нагрузка распределяется между **центральным процессором (CPU)** и **графическим процессором (GPU)**.

В качестве вычислительной операции используется простая и наглядная операция:
[
a_i = a_i \times 2
]

Это позволяет сосредоточиться на сравнении **архитектурных подходов**, а не сложности алгоритма.

### Подход к реализации (Variant 1)

Вариант 1 основан на **статическом разбиении данных**:

* первая половина массива обрабатывается на CPU;
* вторая половина массива обрабатывается на GPU.

Разделение массива:
[
split = \frac{N}{2}
]

### Реализация CPU

* Обработка выполняется последовательно в одном потоке.
* Каждый элемент массива умножается на 2.
* Используется стандартный цикл `for`.
* Время выполнения измеряется с помощью `std::chrono`.

### Реализация GPU

* Для обработки используется CUDA-ядро `processKernel`.
* Каждый CUDA-поток обрабатывает один элемент массива.
* Используется стандартная схема индексации:

  ```
  gid = blockIdx.x * blockDim.x + threadIdx.x
  ```
* Замер времени осуществляется с помощью **CUDA Events**.

### Гибридная реализация (CPU + GPU)

Гибридный алгоритм состоит из двух параллельных этапов:

#### Этап 1: CPU

* CPU обрабатывает первую половину массива `[0, split)`.
* Обработка выполняется последовательно.

#### Этап 2: GPU

* GPU обрабатывает вторую половину массива `[split, N)`.
* На видеокарту передаётся только соответствующая часть данных.
* После выполнения ядра результат копируется обратно в хост-память.

Общее время гибридного подхода включает:

* вычисления на CPU;
* копирование данных между CPU и GPU;
* выполнение CUDA-ядра.

### Используемые технологии

* **CUDA C++**
* **Hybrid CPU + GPU computing**
* **CUDA Kernels**
* **CUDA Events**
* **C++ STL** (`vector`, `chrono`, `random`)

### Параметры эксперимента

| Параметр          | Значение          |
| ----------------- | ----------------- |
| Размер массива    | 1 000 000         |
| Размер блока CUDA | 256               |
| Тип разбиения     | 50% CPU / 50% GPU |
| Тип данных        | `int`             |
| Операция          | умножение на 2    |


### Проверка корректности

Корректность реализации проверяется путём:

* вывода первых и последних 10 элементов исходного массива;
* сравнения результатов CPU, GPU и гибридной реализаций.

Все три подхода должны давать **идентичные результаты**.

### Результаты и сравнение

В ходе эксперимента измеряются:

* время выполнения CPU-реализации;
* время выполнения GPU-реализации;
* общее время гибридной обработки.

Ожидаемые наблюдения:

* GPU-реализация демонстрирует ускорение за счёт массового параллелизма;
* гибридный подход может быть менее эффективным для простой операции из-за накладных расходов на передачу данных;
* гибридные вычисления наиболее эффективны для **вычислительно сложных задач**.

### Вывод

В рамках задания был реализован гибридный алгоритм обработки массива, использующий совместную работу CPU и GPU. Эксперимент показал, что гибридный подход не всегда превосходит чисто GPU-реализацию, особенно при простых операциях, однако он является важной концепцией для задач, где необходимо эффективно распределять вычислительные ресурсы системы.

## Задание 3 (task_3_2.cu)

### Parallel Hybrid CPU + GPU Processing

### Цель задания

Реализовать **параллельную гибридную обработку данных**, при которой **CPU и GPU работают одновременно**, и сравнить производительность:

* чисто CPU-вычислений;
* чисто GPU-вычислений;
* параллельного гибридного подхода (CPU + GPU).

### Описание задачи

В данном варианте исследуется **реальное параллельное взаимодействие CPU и GPU**, где вычисления выполняются **одновременно**, а не последовательно, как в Variant 1.

Для обработки используется операция:
[
a_i = a_i \times 2
]

Размер входного массива составляет **1 000 000 элементов**.

### Подход к реализации (Variant 2)

Вариант 2 основан на **одновременном выполнении вычислений**:

* CPU и GPU обрабатывают разные части массива **параллельно**;
* CPU-вычисления выполняются в отдельном потоке (`std::thread`);
* GPU-вычисления выполняются в основном потоке.

Разделение массива:
[
split = \frac{N}{2}
]


### Реализация CPU

* Первая половина массива `[0, split)` обрабатывается на CPU.
* Обработка выполняется в **отдельном потоке** с использованием `std::thread`.
* Каждый элемент умножается на 2.
* Используется функция `processCPU`.

### Реализация GPU

* Вторая половина массива `[split, N)` обрабатывается на GPU.
* Используется CUDA-ядро `processKernel`.
* Каждый CUDA-поток обрабатывает один элемент массива.
* Выделение памяти и копирование данных производится только для части массива.


### Параллельная гибридная реализация

Гибридный алгоритм состоит из следующих этапов:

1. Запуск CPU-обработки первой половины массива в отдельном потоке.
2. Одновременный запуск CUDA-ядра для второй половины массива.
3. Синхронизация GPU (`cudaDeviceSynchronize`).
4. Ожидание завершения CPU-потока (`thread.join()`).
5. Объединение результатов в одном массиве.

Общее время гибридной обработки включает **реально параллельное выполнение CPU и GPU**.

### Используемые технологии

* **CUDA C++**
* **Hybrid CPU + GPU computing**
* **CUDA Kernels**
* **CUDA Events**
* **C++ Standard Library**

  * `std::vector`
  * `std::thread`
  * `std::chrono`
  * `std::random`

### Параметры эксперимента

| Параметр          | Значение          |
| ----------------- | ----------------- |
| Размер массива    | 1 000 000         |
| Размер блока CUDA | 256               |
| Тип разбиения     | 50% CPU / 50% GPU |
| Тип исполнения    | Параллельное      |
| Тип данных        | `int`             |
| Операция          | умножение на 2    |

### Проверка корректности

Корректность результатов проверяется путём:

* вывода первых и последних 10 элементов исходного массива;
* сравнения результатов CPU, GPU и гибридного методов.

Результаты всех трёх подходов должны совпадать.

### Результаты и анализ

В ходе эксперимента измеряются:

* время CPU-вычислений;
* время GPU-вычислений;
* общее время гибридной параллельной обработки.

Ожидаемые наблюдения:

* GPU значительно быстрее CPU для массивных операций;
* гибридный вариант может быть быстрее чистого GPU, если:

  * CPU и GPU загружены равномерно;
  * накладные расходы на передачу данных минимальны;
* эффективность гибридного подхода зависит от баланса нагрузки.

### Вывод

В рамках задания был реализован параллельный гибридный алгоритм, в котором CPU и GPU работают одновременно. Такой подход демонстрирует преимущества гибридных вычислений при правильном распределении нагрузки и минимальных накладных расходах, и является наиболее приближенным к реальным высокопроизводительным вычислительным системам.

## Задание 4. Распределённые вычисления с использованием MPI

### Описание

В данном задании реализована распределённая параллельная программа с использованием технологии **MPI (Message Passing Interface)**. Глобальный массив данных разбивается на равные части между несколькими процессами. Каждый процесс независимо выполняет вычисления над своей частью данных, после чего результаты собираются на главном процессе. Производится замер времени выполнения для оценки эффективности распределённых вычислений.

### Цель

Целью задания является изучение принципов **распределённых параллельных вычислений**, включая:

* разделение данных между процессами;
* локальную обработку данных в каждом процессе;
* сбор результатов и анализ масштабируемости программы при увеличении числа процессов.

### Функционал

Программа реализует следующий алгоритм:

* инициализация среды MPI и определение количества процессов и ранга каждого процесса;
* разделение глобального массива размером 1 000 000 элементов на равные части между процессами;
* генерация локальных данных в каждом процессе;
* параллельная локальная обработка (умножение элементов на 2);
* синхронизация процессов для корректного измерения времени выполнения;
* сбор обработанных данных на процессе с рангом 0 с использованием `MPI_Gather`;
* вывод части исходного и обработанного массива, а также общего времени выполнения;
* возможность запуска программы с различным числом процессов (2, 4, 8) для анализа производительности.
  
### Технологии

* **MPI (Message Passing Interface)** — организация распределённых вычислений и обмена данными между процессами
* **C++** — реализация основной логики программы
* **MPI_Barrier** — синхронизация процессов
* **MPI_Gather** — сбор данных с всех процессов
* **std::chrono** — измерение времени выполнения
* **STL (std::vector)** — хранение локальных и глобальных массивов

### Контрольные вопросы (`questions.md`)

Файл содержит ответы на вопросы.
