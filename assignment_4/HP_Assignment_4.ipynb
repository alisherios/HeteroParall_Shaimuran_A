{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3atHqVxdIXwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c991a3e2-c752-43f6-d1b8-93d79db3b0d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan 25 12:22:47 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "t_BE7aatEuNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63a1949f-7487-4b71-b5ae-33cf603be3c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 1"
      ],
      "metadata": {
        "id": "yq3SY5dMIzUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile assignment4_task1.cu\n",
        "\n",
        "#include <iostream>        // Стандартный ввод-вывод\n",
        "#include <vector>          // Поддержка контейнера динамических массивов (векторов)\n",
        "#include <chrono>          // Библиотека для точного измерения времени на CPU\n",
        "#include <random>          // Генератор случайных чисел\n",
        "#include <cuda_runtime.h>  // Основной API CUDA для работы с видеокартой\n",
        "\n",
        "#define N 100000           // Размер массива (100 тысяч элементов)\n",
        "#define BLOCK_SIZE 256     // Количество потоков в одном блоке CUDA\n",
        "#define RAND_MIN_VAL -10000 // Минимальное значение случайного числа\n",
        "#define RAND_MAX_VAL 10000  // Максимальное значение случайного числа\n",
        "\n",
        "// CUDA-ядро: функция, которая выполняется параллельно на множестве ядер GPU\n",
        "__global__ void sumKernel(int* d_array, unsigned long long* d_result, int n) {\n",
        "    // Вычисляем глобальный индекс потока\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Проверяем, не выходит ли индекс за пределы массива\n",
        "    if (idx < n) {\n",
        "        // Атомарное сложение: безопасно прибавляет значение элемента к общей сумме.\n",
        "        // atomicAdd гарантирует, что разные потоки не перезапишут данные друг друга одновременно.\n",
        "        atomicAdd(d_result, (unsigned long long)d_array[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // ---------------- ИНИЦИАЛИЗАЦИЯ СЛУЧАЙНОГО МАССИВА ----------------\n",
        "    std::vector<int> h_array(N); // Создаем массив на хосте (CPU) размера N\n",
        "\n",
        "    std::random_device rd;  // Источник энтропии для генератора\n",
        "    std::mt19937 gen(rd()); // Генератор \"Вихрь Мерсенна\"\n",
        "    std::uniform_int_distribution<> dist(RAND_MIN_VAL, RAND_MAX_VAL); // Равномерное распределение чисел\n",
        "\n",
        "    // Заполняем массив случайными числами\n",
        "    for (int i = 0; i < N; i++)\n",
        "        h_array[i] = dist(gen);\n",
        "\n",
        "    // ----------- ВЫВОД ЧАСТИ МАССИВА ДЛЯ ПРОВЕРКИ -----------\n",
        "    std::cout << \"First 10: \";\n",
        "    for (int i = 0; i < 10; i++) std::cout << h_array[i] << \" \";\n",
        "    std::cout << \"\\nLast 10: \";\n",
        "    for (int i = N - 10; i < N; i++) std::cout << h_array[i] << \" \";\n",
        "    std::cout << \"\\n\";\n",
        "\n",
        "    // ---------------- СУММИРОВАНИЕ НА CPU ----------------\n",
        "    long long cpu_sum = 0;\n",
        "    auto cpu_start = std::chrono::high_resolution_clock::now(); // Засекаем время начала\n",
        "\n",
        "    for (int i = 0; i < N; i++)\n",
        "        cpu_sum += h_array[i]; // Обычный цикл суммирования\n",
        "\n",
        "    auto cpu_end = std::chrono::high_resolution_clock::now();   // Засекаем время конца\n",
        "\n",
        "    // Вычисляем длительность в миллисекундах\n",
        "    std::chrono::duration<double, std::milli> cpu_time = cpu_end - cpu_start;\n",
        "\n",
        "    // ---------------- СУММИРОВАНИЕ НА GPU ----------------\n",
        "    int* d_array;                   // Указатель для массива на видеокарте\n",
        "    unsigned long long* d_result;   // Указатель для результата на видеокарте\n",
        "    unsigned long long gpu_sum = 0; // Переменная для хранения итоговой суммы на CPU\n",
        "\n",
        "    // Выделяем память в Глобальной памяти видеокарты (VRAM)\n",
        "    cudaMalloc(&d_array, N * sizeof(int));\n",
        "    cudaMalloc(&d_result, sizeof(unsigned long long));\n",
        "\n",
        "    // Копируем данные из оперативной памяти (Host) в видеопамять (Device)\n",
        "    cudaMemcpy(d_array, h_array.data(), N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    // Обнуляем переменную результата на видеокарте\n",
        "    cudaMemset(d_result, 0, sizeof(unsigned long long));\n",
        "\n",
        "    // Рассчитываем количество блоков: (N / размер_блока) с округлением вверх\n",
        "    int blocks = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\n",
        "    // Создаем события CUDA для измерения времени выполнения ядра\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    cudaEventRecord(start); // Запись события \"старт\"\n",
        "    // Запуск ядра: <<<количество_блоков, потоков_в_блоке>>>\n",
        "    sumKernel<<<blocks, BLOCK_SIZE>>>(d_array, d_result, N);\n",
        "    cudaEventRecord(stop);  // Запись события \"стоп\"\n",
        "\n",
        "    // Копируем результат вычислений обратно с видеокарты на CPU\n",
        "    cudaMemcpy(&gpu_sum, d_result, sizeof(unsigned long long), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Ждем завершения всех операций на GPU перед расчетом времени\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float gpu_time = 0.0f;\n",
        "    // Считаем разницу времени между событиями start и stop\n",
        "    cudaEventElapsedTime(&gpu_time, start, stop);\n",
        "\n",
        "    // ---------------- ВЫВОД РЕЗУЛЬТАТОВ ----------------\n",
        "    std::cout << \"\\nArray size: \" << N << std::endl;\n",
        "    std::cout << \"CPU sum: \" << cpu_sum << std::endl;\n",
        "    std::cout << \"GPU sum: \" << (long long)gpu_sum << std::endl;\n",
        "    std::cout << \"CPU time (ms): \" << cpu_time.count() << std::endl;\n",
        "    std::cout << \"GPU time (ms): \" << gpu_time << std::endl;\n",
        "\n",
        "    // Освобождаем выделенную видеопамять\n",
        "    cudaFree(d_array);\n",
        "    cudaFree(d_result);\n",
        "\n",
        "    return 0; // Завершение программы\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cdMVcICRMYy",
        "outputId": "94501d8e-7cad-4245-b27f-a5fc5fe02c4e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing assignment4_task1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 assignment4_task1.cu -o task1\n",
        "!./task1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_exbujTvQZni",
        "outputId": "acea533a-6bb1-4ba5-d84a-dacfaee32eb4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10: 6717 8088 -9271 -1762 8264 -8391 329 890 1958 5912 \n",
            "Last 10: -4735 -7908 -4457 -5747 8055 -3145 2887 -1930 1777 9958 \n",
            "\n",
            "Array size: 100000\n",
            "CPU sum: -1989944\n",
            "GPU sum: -1989944\n",
            "CPU time (ms): 0.268119\n",
            "GPU time (ms): 0.255904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 2"
      ],
      "metadata": {
        "id": "jputDn83I1TC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile assignment4_task2.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "#include <random>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define N 1000000           // Общее количество элементов в массиве\n",
        "#define BLOCK_SIZE 256      // Количество потоков в одном CUDA-блоке\n",
        "#define RAND_MIN_VAL 1      // Минимальное значение для генерации чисел\n",
        "#define RAND_MAX_VAL 100    // Максимальное значение для генерации чисел\n",
        "\n",
        "// ---------------- Kernel 1: Сканирование на уровне блоков ----------------\n",
        "// Выполняет префиксную сумму внутри каждого блока и сохраняет общую сумму блока\n",
        "__global__ void blockScanKernel(int* d_input, int* d_output, int* d_block_sums, int n) {\n",
        "    // Выделяем разделяемую память (shared memory) для быстрой работы внутри блока\n",
        "    __shared__ int temp[BLOCK_SIZE];\n",
        "\n",
        "    int tid = threadIdx.x;                           // Индекс потока внутри блока\n",
        "    int gid = blockIdx.x * blockDim.x + tid;         // Глобальный индекс потока\n",
        "\n",
        "    // Копируем данные из глобальной памяти в быструю разделяемую память\n",
        "    // Если глобальный индекс за пределами массива, заполняем нулем\n",
        "    temp[tid] = (gid < n) ? d_input[gid] : 0;\n",
        "    __syncthreads();                                 // Ждем, пока все потоки загрузят данные\n",
        "\n",
        "    // Алгоритм инклюзивного сканирования Hillis–Steele\n",
        "    // На каждой итерации шаг (offset) удваивается\n",
        "    for (int offset = 1; offset < blockDim.x; offset <<= 1) {\n",
        "        int val = 0;\n",
        "        if (tid >= offset)                           // Если у потока есть сосед слева на расстоянии offset\n",
        "            val = temp[tid - offset];                // Берем его значение\n",
        "        __syncthreads();                             // Синхронизация перед записью, чтобы не затереть нужные данные\n",
        "        temp[tid] += val;                            // Суммируем\n",
        "        __syncthreads();                             // Синхронизация перед следующей итерацией\n",
        "    }\n",
        "\n",
        "    // Записываем результат внутриблочного сканирования в выходной массив\n",
        "    if (gid < n)\n",
        "        d_output[gid] = temp[tid];\n",
        "\n",
        "    // Последний поток каждого блока сохраняет полную сумму блока в отдельный массив\n",
        "    // Это нужно для того, чтобы потом скорректировать значения в следующих блоках\n",
        "    if (tid == blockDim.x - 1)\n",
        "        d_block_sums[blockIdx.x] = temp[tid];\n",
        "}\n",
        "\n",
        "// ---------------- Kernel 2: Добавление сумм блоков к элементам ----------------\n",
        "// Берет результаты сканирования сумм блоков и прибавляет их к соответствующим блокам\n",
        "__global__ void addBlockSumsKernel(int* d_output, int* d_block_sums_scan, int n) {\n",
        "    int gid = blockIdx.x * blockDim.x + threadIdx.x; // Глобальный индекс потока\n",
        "\n",
        "    // Первый блок (index 0) не нуждается в коррекции, также проверяем границы массива\n",
        "    if (blockIdx.x == 0 || gid >= n) return;\n",
        "\n",
        "    // Берем префиксную сумму всех предыдущих блоков\n",
        "    int add_val = d_block_sums_scan[blockIdx.x - 1];\n",
        "    // Прибавляем эту сумму к текущему элементу\n",
        "    d_output[gid] += add_val;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // ---------------- ИНИЦИАЛИЗАЦИЯ МАССИВОВ ----------------\n",
        "    // Хост-векторы для входных данных, результата CPU и результата GPU\n",
        "    std::vector<int> h_input(N), h_output_cpu(N), h_output_gpu(N);\n",
        "\n",
        "    // Настройка генератора случайных чисел\n",
        "    std::random_device rd;\n",
        "    std::mt19937 gen(rd());\n",
        "    std::uniform_int_distribution<> dist(RAND_MIN_VAL, RAND_MAX_VAL);\n",
        "\n",
        "    // Заполнение входного массива случайными числами\n",
        "    for (int i = 0; i < N; i++)\n",
        "        h_input[i] = dist(gen);\n",
        "\n",
        "    // ---------------- ПРЕФИКСНАЯ СУММА НА CPU ----------------\n",
        "    auto cpu_start = std::chrono::high_resolution_clock::now(); // Замер времени начала\n",
        "    h_output_cpu[0] = h_input[0];\n",
        "    for (int i = 1; i < N; i++)\n",
        "        h_output_cpu[i] = h_output_cpu[i - 1] + h_input[i]; // Последовательный алгоритм\n",
        "    auto cpu_end = std::chrono::high_resolution_clock::now();   // Замер времени окончания\n",
        "    std::chrono::duration<double, std::milli> cpu_time = cpu_end - cpu_start;\n",
        "\n",
        "    // ---------------- ПРЕФИКСНАЯ СУММА НА GPU ----------------\n",
        "    int *d_input, *d_output, *d_block_sums, *d_block_sums_scan;\n",
        "    int numBlocks = (N + BLOCK_SIZE - 1) / BLOCK_SIZE; // Вычисляем необходимое кол-во блоков\n",
        "\n",
        "    // Выделение видеопамяти под входные, выходные данные и промежуточные суммы блоков\n",
        "    cudaMalloc(&d_input, N * sizeof(int));\n",
        "    cudaMalloc(&d_output, N * sizeof(int));\n",
        "    cudaMalloc(&d_block_sums, numBlocks * sizeof(int));\n",
        "    cudaMalloc(&d_block_sums_scan, numBlocks * sizeof(int));\n",
        "\n",
        "    // Копирование входных данных с хоста (RAM) на девайс (VRAM)\n",
        "    cudaMemcpy(d_input, h_input.data(), N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Создание событий CUDA для точного замера времени выполнения ядер\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaEventRecord(start); // Фиксация времени старта\n",
        "\n",
        "    // ЭТАП 1: Запуск ядра для локального сканирования внутри каждого блока\n",
        "\n",
        "    blockScanKernel<<<numBlocks, BLOCK_SIZE>>>(d_input, d_output, d_block_sums, N);\n",
        "\n",
        "    // ЭТАП 2: Сканирование сумм блоков\n",
        "    // В данном коде это делается на CPU, так как количество блоков значительно меньше N\n",
        "    std::vector<int> h_block_sums(numBlocks);\n",
        "    // Копируем частичные суммы блоков обратно на CPU\n",
        "    cudaMemcpy(h_block_sums.data(), d_block_sums, numBlocks * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    std::vector<int> h_block_sums_scan(numBlocks);\n",
        "    h_block_sums_scan[0] = h_block_sums[0];\n",
        "    for (int i = 1; i < numBlocks; i++)\n",
        "        h_block_sums_scan[i] = h_block_sums_scan[i - 1] + h_block_sums[i]; // Префиксная сумма самих блоков\n",
        "\n",
        "    // Копируем отсканированные суммы блоков обратно на GPU\n",
        "    cudaMemcpy(d_block_sums_scan, h_block_sums_scan.data(), numBlocks * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // ЭТАП 3: Финальная коррекция\n",
        "    // Добавляем накопленную сумму предыдущих блоков к элементам текущего блока\n",
        "    addBlockSumsKernel<<<numBlocks, BLOCK_SIZE>>>(d_output, d_block_sums_scan, N);\n",
        "\n",
        "    // Копируем финальный результат с GPU на CPU\n",
        "    cudaMemcpy(h_output_gpu.data(), d_output, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaEventRecord(stop);      // Фиксация времени окончания\n",
        "    cudaEventSynchronize(stop); // Ожидание завершения всех операций на GPU\n",
        "\n",
        "    float gpu_time = 0.0f;\n",
        "    cudaEventElapsedTime(&gpu_time, start, stop); // Расчет затраченного времени в мс\n",
        "\n",
        "    // ---------------- ВЫВОД РЕЗУЛЬТАТОВ ----------------\n",
        "    int print_count = 10;\n",
        "    std::cout << \"\\nInput array - first \" << print_count << \": \";\n",
        "    for (int i = 0; i < print_count; i++) std::cout << h_input[i] << \" \";\n",
        "    std::cout << \"\\nInput array - last \" << print_count << \": \";\n",
        "    for (int i = N - print_count; i < N; i++) std::cout << h_input[i] << \" \";\n",
        "\n",
        "    std::cout << \"\\n\\nCPU prefix sum - first \" << print_count << \": \";\n",
        "    for (int i = 0; i < print_count; i++) std::cout << h_output_cpu[i] << \" \";\n",
        "    std::cout << \"\\nCPU prefix sum - last \" << print_count << \": \";\n",
        "    for (int i = N - print_count; i < N; i++) std::cout << h_output_cpu[i] << \" \";\n",
        "\n",
        "    std::cout << \"\\n\\nGPU prefix sum - first \" << print_count << \": \";\n",
        "    for (int i = 0; i < print_count; i++) std::cout << h_output_gpu[i] << \" \";\n",
        "    std::cout << \"\\nGPU prefix sum - last \" << print_count << \": \";\n",
        "    for (int i = N - print_count; i < N; i++) std::cout << h_output_gpu[i] << \" \";\n",
        "\n",
        "    std::cout << \"\\n\\nArray size: \" << N << std::endl;\n",
        "    std::cout << \"CPU time (ms): \" << cpu_time.count() << std::endl;\n",
        "    std::cout << \"GPU time (ms): \" << gpu_time << std::endl;\n",
        "\n",
        "    // Освобождение выделенной видеопамяти\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "    cudaFree(d_block_sums);\n",
        "    cudaFree(d_block_sums_scan);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9kkmICnjYfZ",
        "outputId": "788a76b0-7ac5-47ab-f246-84577bfa79c0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting assignment4_task2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 assignment4_task2.cu -o task2\n",
        "!./task2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDe2kCOfjZCO",
        "outputId": "4374b0ff-4cf4-4c36-e8ec-55b672f2992a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input array - first 10: 11 91 78 20 59 29 77 100 24 3 \n",
            "Input array - last 10: 11 30 2 23 46 46 81 90 19 48 \n",
            "\n",
            "CPU prefix sum - first 10: 11 102 180 200 259 288 365 465 489 492 \n",
            "CPU prefix sum - last 10: 50497629 50497659 50497661 50497684 50497730 50497776 50497857 50497947 50497966 50498014 \n",
            "\n",
            "GPU prefix sum - first 10: 11 102 180 200 259 288 365 465 489 492 \n",
            "GPU prefix sum - last 10: 50497629 50497659 50497661 50497684 50497730 50497776 50497857 50497947 50497966 50498014 \n",
            "\n",
            "Array size: 1000000\n",
            "CPU time (ms): 7.21465\n",
            "GPU time (ms): 1.31226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 3"
      ],
      "metadata": {
        "id": "j0INByRqI32p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile assignment4_task3.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "#include <random>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define N 1000000           // Общее количество элементов в массиве (1 миллион)\n",
        "#define BLOCK_SIZE 256      // Количество потоков в одном CUDA-блоке\n",
        "#define RAND_MIN_VAL -1000  // Нижняя граница для генерации случайных чисел\n",
        "#define RAND_MAX_VAL 1000   // Верхняя граница для генерации случайных чисел\n",
        "\n",
        "// ---------------- GPU Kernel ----------------\n",
        "// Ядро CUDA: выполняется параллельно на видеокарте\n",
        "__global__ void processKernel(int* d_array, int n) {\n",
        "    // Вычисляем уникальный глобальный индекс потока\n",
        "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Проверяем, не выходит ли индекс за пределы массива\n",
        "    if (gid < n) {\n",
        "        // Выполняем полезную работу: умножаем значение элемента на 2\n",
        "        d_array[gid] *= 2;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // ---------------- ИНИЦИАЛИЗАЦИЯ МАССИВОВ ----------------\n",
        "    // Резервируем память на хосте (CPU) для исходного массива и результатов разных типов обработки\n",
        "    std::vector<int> h_array(N), h_cpu(N), h_gpu(N), h_hybrid(N);\n",
        "\n",
        "    // Подготовка генератора случайных чисел\n",
        "    std::random_device rd;\n",
        "    std::mt19937 gen(rd());\n",
        "    std::uniform_int_distribution<> dist(RAND_MIN_VAL, RAND_MAX_VAL);\n",
        "\n",
        "    // Заполнение исходного массива случайными данными\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_array[i] = dist(gen);\n",
        "    }\n",
        "\n",
        "    // ---------------- ОБРАБОТКА НА CPU ----------------\n",
        "    h_cpu = h_array; // Копируем данные в массив для обработки на процессоре\n",
        "    auto cpu_start = std::chrono::high_resolution_clock::now(); // Засекаем время начала\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_cpu[i] *= 2; // Последовательное умножение каждого элемента\n",
        "    }\n",
        "    auto cpu_end = std::chrono::high_resolution_clock::now();   // Засекаем время окончания\n",
        "    // Вычисляем длительность в миллисекундах\n",
        "    std::chrono::duration<double, std::milli> cpu_time = cpu_end - cpu_start;\n",
        "\n",
        "    // ---------------- ОБРАБОТКА НА GPU ----------------\n",
        "    h_gpu = h_array; // Подготовка массива для GPU-теста\n",
        "    int* d_array;    // Указатель для памяти на видеокарте\n",
        "\n",
        "    // Выделяем память в VRAM и копируем туда данные с хоста\n",
        "    cudaMalloc(&d_array, N * sizeof(int));\n",
        "    cudaMemcpy(d_array, h_gpu.data(), N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Расчет количества блоков, необходимых для покрытия всего массива N\n",
        "    int numBlocks = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\n",
        "    // Настройка событий CUDA для точного замера времени работы видеокарты\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaEventRecord(start); // Старт записи времени\n",
        "\n",
        "    // Запуск вычислительного ядра на GPU\n",
        "    processKernel<<<numBlocks, BLOCK_SIZE>>>(d_array, N);\n",
        "\n",
        "    cudaEventRecord(stop);      // Остановка записи времени\n",
        "    cudaEventSynchronize(stop); // Ждем завершения всех операций на GPU\n",
        "\n",
        "    // Копируем обработанные данные обратно в оперативную память (RAM)\n",
        "    cudaMemcpy(h_gpu.data(), d_array, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float gpu_time = 0.0f;\n",
        "    cudaEventElapsedTime(&gpu_time, start, stop); // Получаем время работы в мс\n",
        "    cudaFree(d_array); // Освобождаем память на видеокарте\n",
        "\n",
        "    // ---------------- ГИБРИДНАЯ ОБРАБОТКА (CPU + GPU) ----------------\n",
        "    int split = N / 2;  // Определяем точку разделения: половина на CPU, половина на GPU\n",
        "    h_hybrid = h_array; // Копируем исходные данные\n",
        "\n",
        "    // Фиксируем время начала всей гибридной операции\n",
        "    auto hybrid_start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Часть 1: CPU обрабатывает первую половину массива\n",
        "    for (int i = 0; i < split; i++) {\n",
        "        h_hybrid[i] *= 2;\n",
        "    }\n",
        "\n",
        "    // Часть 2: GPU обрабатывает вторую половину массива\n",
        "    int* d_hybrid;\n",
        "    // Выделяем память только под оставшуюся половину (N - split)\n",
        "    cudaMalloc(&d_hybrid, (N - split) * sizeof(int));\n",
        "    // Копируем вторую часть данных на девайс, начиная со смещения split\n",
        "    cudaMemcpy(d_hybrid, &h_hybrid[split], (N - split) * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Рассчитываем блоки для гибридной части\n",
        "    int blocks_hybrid = ((N - split) + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\n",
        "    // Запуск ядра для обработки второй половины\n",
        "    processKernel<<<blocks_hybrid, BLOCK_SIZE>>>(d_hybrid, N - split);\n",
        "    cudaDeviceSynchronize(); // Синхронизация устройства\n",
        "\n",
        "    // Копируем результат GPU-части обратно в массив h_hybrid по нужному адресу\n",
        "    cudaMemcpy(&h_hybrid[split], d_hybrid, (N - split) * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    auto hybrid_end = std::chrono::high_resolution_clock::now(); // Конец гибридной обработки\n",
        "\n",
        "    // Рассчитываем общее время гибридного подхода (включая копирование и работу CPU)\n",
        "    float hybrid_time = std::chrono::duration<double, std::milli>(hybrid_end - hybrid_start).count();\n",
        "    cudaFree(d_hybrid); // Освобождаем выделенную память\n",
        "\n",
        "    // ---------------- ПРОВЕРКА И ВЫВОД РЕЗУЛЬТАТОВ ----------------\n",
        "    int print_count = 10; // Количество элементов для предпросмотра\n",
        "\n",
        "    // Вывод первых и последних элементов исходного массива\n",
        "    std::cout << \"Original array - first \" << print_count << \": \";\n",
        "    for (int i = 0; i < print_count; i++) std::cout << h_array[i] << \" \";\n",
        "    std::cout << \"\\nOriginal array - last \" << print_count << \": \";\n",
        "    for (int i = N - print_count; i < N; i++) std::cout << h_array[i] << \" \";\n",
        "\n",
        "    // Вывод результатов CPU\n",
        "    std::cout << \"\\n\\nCPU processed - first \" << print_count << \": \";\n",
        "    for (int i = 0; i < print_count; i++) std::cout << h_cpu[i] << \" \";\n",
        "    std::cout << \"\\nCPU processed - last \" << print_count << \": \";\n",
        "    for (int i = N - print_count; i < N; i++) std::cout << h_cpu[i] << \" \";\n",
        "\n",
        "    // Вывод результатов GPU\n",
        "    std::cout << \"\\n\\nGPU processed - first \" << print_count << \": \";\n",
        "    for (int i = 0; i < print_count; i++) std::cout << h_gpu[i] << \" \";\n",
        "    std::cout << \"\\nGPU processed - last \" << print_count << \": \";\n",
        "    for (int i = N - print_count; i < N; i++) std::cout << h_gpu[i] << \" \";\n",
        "\n",
        "    // Вывод результатов гибридного метода\n",
        "    std::cout << \"\\n\\nHybrid processed - first \" << print_count << \": \";\n",
        "    for (int i = 0; i < print_count; i++) std::cout << h_hybrid[i] << \" \";\n",
        "    std::cout << \"\\nHybrid processed - last \" << print_count << \": \";\n",
        "    for (int i = N - print_count; i < N; i++) std::cout << h_hybrid[i] << \" \";\n",
        "\n",
        "    // ---------------- СРАВНЕНИЕ ВРЕМЕНИ ----------------\n",
        "    std::cout << \"\\n\\nArray size: \" << N << std::endl;\n",
        "    std::cout << \"CPU time (ms): \" << cpu_time.count() << std::endl;\n",
        "    std::cout << \"GPU time (ms): \" << gpu_time << std::endl;\n",
        "    std::cout << \"Hybrid time (ms): \" << hybrid_time << std::endl;\n",
        "\n",
        "    return 0; // Завершение программы\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqpvVeCWqj55",
        "outputId": "5c86c7bd-560c-4bac-c68c-18cf2453e56d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting assignment4_task3.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 assignment4_task3.cu -o task3\n",
        "!./task3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT5PjTiaqIGD",
        "outputId": "35dc9c0a-e90b-498d-f2b5-385bf2b53df4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original array - first 10: -119 481 -75 732 -686 846 678 925 -919 -561 \n",
            "Original array - last 10: -250 -958 -186 848 -452 957 -16 -602 -431 934 \n",
            "\n",
            "CPU processed - first 10: -238 962 -150 1464 -1372 1692 1356 1850 -1838 -1122 \n",
            "CPU processed - last 10: -500 -1916 -372 1696 -904 1914 -32 -1204 -862 1868 \n",
            "\n",
            "GPU processed - first 10: -238 962 -150 1464 -1372 1692 1356 1850 -1838 -1122 \n",
            "GPU processed - last 10: -500 -1916 -372 1696 -904 1914 -32 -1204 -862 1868 \n",
            "\n",
            "Hybrid processed - first 10: -238 962 -150 1464 -1372 1692 1356 1850 -1838 -1122 \n",
            "Hybrid processed - last 10: -500 -1916 -372 1696 -904 1914 -32 -1204 -862 1868 \n",
            "\n",
            "Array size: 1000000\n",
            "CPU time (ms): 4.94592\n",
            "GPU time (ms): 0.131456\n",
            "Hybrid time (ms): 3.6416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile assignment4_task3_hybrid.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "#include <random>\n",
        "#include <thread>           // Библиотека для работы с потоками CPU (std::thread)\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define N 1000000           // Размер массива\n",
        "#define BLOCK_SIZE 256      // Потоков на блок\n",
        "#define RAND_MIN_VAL -1000\n",
        "#define RAND_MAX_VAL 1000\n",
        "\n",
        "// ---------------- GPU Kernel ----------------\n",
        "// Ядро для параллельных вычислений на видеокарте\n",
        "__global__ void processKernel(int* d_array, int n) {\n",
        "    // Вычисляем глобальный индекс текущего потока\n",
        "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    // Проверка выхода за границы массива\n",
        "    if (gid < n) {\n",
        "        d_array[gid] *= 2; // Умножаем элемент на 2\n",
        "    }\n",
        "}\n",
        "\n",
        "// ---------------- CPU processing function ----------------\n",
        "// Функция для обработки данных на CPU (будет запущена в отдельном потоке)\n",
        "void processCPU(std::vector<int>& arr, int start, int end) {\n",
        "    // Обработка заданного диапазона [start, end)\n",
        "    for (int i = start; i < end; i++) {\n",
        "        arr[i] *= 2;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // ---------------- INIT ARRAY ----------------\n",
        "    // Подготовка векторов для хранения исходных данных и результатов разных тестов\n",
        "    std::vector<int> h_array(N), h_cpu(N), h_gpu(N), h_hybrid(N);\n",
        "\n",
        "    std::random_device rd;\n",
        "    std::mt19937 gen(rd());\n",
        "    std::uniform_int_distribution<> dist(RAND_MIN_VAL, RAND_MAX_VAL);\n",
        "\n",
        "    // Инициализация исходного массива случайными числами\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_array[i] = dist(gen);\n",
        "    }\n",
        "\n",
        "    // ---------------- CPU PROCESS ----------------\n",
        "    h_cpu = h_array; // Копируем данные для чистого CPU-теста\n",
        "    auto cpu_start = std::chrono::high_resolution_clock::now();\n",
        "    processCPU(h_cpu, 0, N); // Обрабатываем весь массив на CPU последовательно\n",
        "    auto cpu_end = std::chrono::high_resolution_clock::now();\n",
        "    // Расчет времени выполнения на CPU\n",
        "    std::chrono::duration<double, std::milli> cpu_time = cpu_end - cpu_start;\n",
        "\n",
        "    // ---------------- GPU PROCESS ----------------\n",
        "    h_gpu = h_array; // Копируем данные для чистого GPU-теста\n",
        "    int* d_array;\n",
        "    // Выделяем память на видеокарте и копируем туда весь массив\n",
        "    cudaMalloc(&d_array, N * sizeof(int));\n",
        "    cudaMemcpy(d_array, h_gpu.data(), N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Считаем количество блоков для покрытия N элементов\n",
        "    int numBlocks = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\n",
        "    // Создание и фиксация событий для замера времени GPU\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    // Запуск ядра для обработки всего массива на GPU\n",
        "    processKernel<<<numBlocks, BLOCK_SIZE>>>(d_array, N);\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop); // Ждем завершения расчетов\n",
        "\n",
        "    // Копируем результат обратно в h_gpu\n",
        "    cudaMemcpy(h_gpu.data(), d_array, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float gpu_time = 0.0f;\n",
        "    cudaEventElapsedTime(&gpu_time, start, stop); // Получаем время работы GPU в мс\n",
        "    cudaFree(d_array); // Освобождаем память на видеокарте\n",
        "\n",
        "    // ---------------- PARALLEL HYBRID PROCESS ----------------\n",
        "\n",
        "    h_hybrid = h_array; // Копируем данные для гибридного теста\n",
        "    int split = N / 2;  // Точка разделения нагрузки пополам\n",
        "\n",
        "    auto hybrid_start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // ЗАПУСК ПОТОКА CPU: обрабатывает первую половину массива [0, split)\n",
        "    // std::ref используется для передачи вектора по ссылке\n",
        "    std::thread cpu_thread(processCPU, std::ref(h_hybrid), 0, split);\n",
        "\n",
        "    // ЗАПУСК GPU В ОСНОВНОМ ПОТОКЕ: обрабатывает вторую половину [split, N)\n",
        "    int* d_hybrid;\n",
        "    // Выделяем память на GPU только под вторую половину\n",
        "    cudaMalloc(&d_hybrid, (N - split) * sizeof(int));\n",
        "    // Копируем вторую половину данных на GPU\n",
        "    cudaMemcpy(d_hybrid, &h_hybrid[split], (N - split) * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Расчет количества блоков для половины массива\n",
        "    int blocks_hybrid = ((N - split) + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "    // Вызываем ядро для обработки половины данных на GPU\n",
        "    processKernel<<<blocks_hybrid, BLOCK_SIZE>>>(d_hybrid, N - split);\n",
        "    cudaDeviceSynchronize(); // Ожидаем завершения вычислений на видеокарте\n",
        "\n",
        "    // Копируем результат GPU-части обратно в исходный вектор на хосте\n",
        "    cudaMemcpy(&h_hybrid[split], d_hybrid, (N - split) * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaFree(d_hybrid); // Освобождаем память GPU\n",
        "\n",
        "    // СИНХРОНИЗАЦИЯ: Ждем, пока поток CPU завершит свою работу над первой половиной\n",
        "    cpu_thread.join();\n",
        "\n",
        "    auto hybrid_end = std::chrono::high_resolution_clock::now();\n",
        "    // Расчет общего времени гибридной обработки (параллельное исполнение)\n",
        "    float hybrid_time = std::chrono::duration<double, std::milli>(hybrid_end - hybrid_start).count();\n",
        "\n",
        "    // ---------------- CHECK RESULTS (Вывод результатов) ----------------\n",
        "    int print_count = 10;\n",
        "    std::cout << \"Original array - first \" << print_count << \": \";\n",
        "    for (int i = 0; i < print_count; i++) std::cout << h_array[i] << \" \";\n",
        "    std::cout << \"\\nOriginal array - last \" << print_count << \": \";\n",
        "    for (int i = N - print_count; i < N; i++) std::cout << h_array[i] << \" \";\n",
        "\n",
        "    std::cout << \"\\n\\nCPU processed - first \" << print_count << \": \";\n",
        "    for (int i = 0; i < print_count; i++) std::cout << h_cpu[i] << \" \";\n",
        "    std::cout << \"\\nCPU processed - last \" << print_count << \": \";\n",
        "    for (int i = N - print_count; i < N; i++) std::cout << h_cpu[i] << \" \";\n",
        "\n",
        "    std::cout << \"\\n\\nGPU processed - first \" << print_count << \": \";\n",
        "    for (int i = 0; i < print_count; i++) std::cout << h_gpu[i] << \" \";\n",
        "    std::cout << \"\\nGPU processed - last \" << print_count << \": \";\n",
        "    for (int i = N - print_count; i < N; i++) std::cout << h_gpu[i] << \" \";\n",
        "\n",
        "    std::cout << \"\\n\\nHybrid processed - first \" << print_count << \": \";\n",
        "    for (int i = 0; i < print_count; i++) std::cout << h_hybrid[i] << \" \";\n",
        "    std::cout << \"\\nHybrid processed - last \" << print_count << \": \";\n",
        "    for (int i = N - print_count; i < N; i++) std::cout << h_hybrid[i] << \" \";\n",
        "\n",
        "    // ---------------- TIME RESULTS (Вывод временных показателей) ----------------\n",
        "    std::cout << \"\\n\\nArray size: \" << N << std::endl;\n",
        "    std::cout << \"CPU time (ms): \" << cpu_time.count() << std::endl;\n",
        "    std::cout << \"GPU time (ms): \" << gpu_time << std::endl;\n",
        "    std::cout << \"Hybrid time (ms): \" << hybrid_time << std::endl;\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO2Z2BVmuNcf",
        "outputId": "3bbd4ad7-6db6-45d3-9f9a-a09cc65bc45a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing assignment4_task3_hybrid.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 assignment4_task3_hybrid.cu -o task3\n",
        "!./task3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n38KoeH1uTsR",
        "outputId": "69a0c07f-94df-4314-ed54-7ca3be724fd5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original array - first 10: 649 -850 214 899 776 -688 -359 -922 -245 473 \n",
            "Original array - last 10: -998 781 484 -341 -876 111 640 794 856 621 \n",
            "\n",
            "CPU processed - first 10: 1298 -1700 428 1798 1552 -1376 -718 -1844 -490 946 \n",
            "CPU processed - last 10: -1996 1562 968 -682 -1752 222 1280 1588 1712 1242 \n",
            "\n",
            "GPU processed - first 10: 1298 -1700 428 1798 1552 -1376 -718 -1844 -490 946 \n",
            "GPU processed - last 10: -1996 1562 968 -682 -1752 222 1280 1588 1712 1242 \n",
            "\n",
            "Hybrid processed - first 10: 1298 -1700 428 1798 1552 -1376 -718 -1844 -490 946 \n",
            "Hybrid processed - last 10: -1996 1562 968 -682 -1752 222 1280 1588 1712 1242 \n",
            "\n",
            "Array size: 1000000\n",
            "CPU time (ms): 3.36086\n",
            "GPU time (ms): 0.12288\n",
            "Hybrid time (ms): 2.60477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4LLXuCdcGONl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}